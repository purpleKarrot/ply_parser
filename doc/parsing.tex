\documentclass[a4paper,parskip=half,twocolumn]{scrartcl}
%
\usepackage{geometry}
\geometry{a4paper,left=25mm,right=25mm, top=25mm, bottom=25mm} 
\setlength{\columnsep}{10mm}

%\usepackage{a4wide}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}

\usepackage[numbers]{natbib}

\usepackage{listings}
\lstset{basicstyle=\ttfamily\scriptsize\mdseries}

\usepackage{hyperref}
\hypersetup{colorlinks=true}
\urlstyle{same}

\title{Type-Safe Parsing with a Dynamic Grammar}

\author{Daniel Pfeifer,
\href{mailto:daniel@pfeifer-mail.de}{daniel@pfeifer-mail.de} \\
Department of Informatics (IFI), University of Zurich}

\date{December 31, 2010}

\begin{document}

\maketitle

\begin{abstract}
In computer science, parsing is the process of analyzing a sequence of tokens
(words, bytes or even single bits) with respect to a formal grammar. The way
the analyzed data is processed is called semantics.

Parsers may be crafted by hand or generated by a tool that generates source code
from specification in a formal language. Both of these approaches require a
complete knowlegde of the data to be parsed.

In this paper I am going to present an approach of building a parser from a
dynamic grammar and static semantics. This allows to specify the data to be
parsed at runtime, but still provides static type safety. As an example
application, I wrote a parser for the PLY file format using the techniques
described in this paper.
\end{abstract}

\section{Motivation}
In the context of data storage and transmission, parsing plays a fundamental
role. Whenever a file is loaded from a disk or a network protocol is
interpreted, the file or stream content needs to be parsed and converted into a
data structure or object.

In simple cases, the file content is an exact representation of the data
structure. More general file formats and protocols provide a \emph{header}
information that describes the content of the \emph{body}.

A general parser should be able to parse the body of a file or network packet,
with respect to the information from the respective header, thus generate a
formal grammar by parsing the header. Because this grammar is constructed at
runtime, let us call it a \emph{dynamic grammar}.

The application developer that uses a parser library specifies a class or data
structure that should be initialized by the parser. An example data structure is
given in Listing \ref{vertex}. This data structure directly affects the
semantics of the parser.

\begin{lstlisting}[frame=tb,label=vertex,caption=Vertex Example]
struct vertex
{
  float x, y, z;
  uchar b, g, r;
};
\end{lstlisting}

The parser has to initialize this structure, no matter what the contents of the
file really are. Because this structure defines a type and C++ is a statically
typed language, the semantics of the parser is static.

\subsection{Example}
We have a file that contains a list of vertices. Each vertex consists of a
position and a color. The position is given as two double precision floating
point numbers and the color is in ARGB format, where each color channel is an
unsigned char. The list of vertices should be parsed, and written into an array
of the vertex struct introduced in Listing \ref{vertex}.

The parser should be able to match these seven numbers in the format that is
specified in the header, convert the first two numbers to float, assign the
values to the corresponding elements of the struct (taking reordering into
account), set the element z to a default and discard the alpha channel of the
color.

\section{Used Libraries}
There are generally three ways to generate a parser from a formal grammar.
\begin{itemize}
  \item hand crafted
  \item code generator
  \item DSEL
\end{itemize}
The grammar could be passed to a team of programmers that craft the parser by
hand, following the specification. This approach is error prone and requires
acceptance testing. The whole generation can be automated by a tool that
generates source code from a formal grammar (e.g. Lex/YACC). Yet another
approach is to embed the formal grammar directly into the source code in a
domain specific embedded language (e.g. Qi from the Boost.Spirit library).

Since our knowledge about the grammar is deferred until runtime, we cannot
depend on generated source code, hand crafted or not. The following approach
uses Qi from the Boost.Spirit library. One improtant feature of Qi is that it
allows to reassign rules as shown in listing \ref{reassign}. In the second line
we take a rule, copy it\footnote{otherwise this operation generates a recursive
parser}, append another terminal and assign the resulting parser to rule,
overiding its previous value. 

\begin{lstlisting}[frame=tb,label=reassign,caption=reassigning rules]
RULE ::= TEMINAL1
RULE ::= RULE' TERMINAL2
\end{lstlisting}

The following approach also uses Boost.Fusion and Boost.MPL for template
metaprogramming. Further, some functional programming with Boost.Phoenix is
required.

\section{Approach}
The grammar will be generated from two components. The first component is the
type which the parser will return. We call this type \texttt{Output}, starting
with a capital letter to denote it is a generic type. \texttt{Output} is
required to model the \texttt{Sequence} Concept defined by the Boost.Fusion
library to allow us to inspect its elements with template metaprogramming.

The second component is a dynamically sized list of \texttt{struct element}
shown in listing \ref{element}. This list is usually filled by parsing the
header information. We call this list \texttt{input}, in all lower case, to
denote that its type is specificied.

\begin{lstlisting}[frame=tb,label=element,caption=element type]
struct element
{
  enum { efloat, edouble, eint, ... } type; 
  std::string name;
};
typedef std::vector<element> input;
\end{lstlisting}

For the grammar, we use \texttt{Output} as a template argument and pass
\texttt{input} to the constructor. Internally, we create a subrule for each
element of \texttt{input}. The subrule will match a type depending on the
\texttt{type} enumerator. It will use a semantic actions depending on
\texttt{Output}. These subrules will be chained by appending them to the
starting rule as shown in listing \ref{reassign} to build an appropriate
grammar.

As said before, Qi allows us to reassign rules. However, rules hold terminals
and subrules by reference, so it is not possible to return subrules from a
factory function, which would be the preferred method to generate objects
depending on an enumerated value. Further, semantic actions require the types
of the rules to be specified at compile time.

The grammar needs to hold all rules that might require a semantic action by
value. This is exactly one rule per element in \texttt{input} plus one for each
element in \texttt{Output} whose name matches an element in \texttt{input}.
Wether the names match is unknown at compile time, so we need to provide rules
for all elements of \texttt{Output}, accepting the fact that they might not be
used.

The amount of rules depending on \texttt{input} is dynamic, so we generate an
array of rules at runtime. Each of these rules may or may not initialize any
element of \texttt{Output}. We therefore use \texttt{void(Output\&)} as a
signature for all of these rules.

To instanciate the rules for the elements of \texttt{Output}, we transform
the \texttt{Output} sequence into a rule sequence that contains a rule with a
signature of \texttt{void(T\&)} for each type \texttt{T} in \texttt{Output}.

The actual grammar construction, which is happening at runtime, is presented in
listing \ref{algorithm} as pseudo code.

\begin{lstlisting}[frame=tb,label=algorithm,caption=grammar construction algorithm]
init grammar
foreach element i in input
  assign omit parser
  foreach o in Output
    if i.name equals o.name
      reassign element parser
    endif
  endforeach
  chain rule
endforeach
finalize grammar
\end{lstlisting}

The grammar is initialized with an empty rule. 


The grammar generation can be completely abstracted from the application
developer. The information about where the result should be written to, gives
the parser library the complete information about the type of the expected
result. There is no need to require the application developer to register
the structure and its elements to the parser library.






The resulting grammar is used to create a

We start it with an uppercase letter to denote it is a type. 

We say it has a signature of \texttt{Output()}.




The PLY file format was developed at Stanford University. The original goal of
the PLY format was to ``provide a format that is simple and easy to implement,
but that is general enough to be useful for a wide range of models''. The format
is actually general to such an extent, that it is not possible to make any
assertion about the content of a file until the file header has been parsed.
Consequently, it is impossible to support the full PLY format in a type-safe
manner.

Writing a loader for a given PLY file is a relatively easy task. The types of
the content are known and the file header is used merely as a check that the
right file is opened. This loader can provide very high efficiency, but fails
loading any other file.

Implementing a PLY loader as a reusable library can be achieved in two ways:
\begin{enumerate}
  \item All data is allocated depending on the information in the file header.
  The client application has to interpret the data at runtime, e.g. in a dynamic
  rendering routine. In applications where performance is critical, a dynamic
  rendering routine is simply not acceptable.
  \item The programmer ``tells'' the library what data she wants and where it
  should be written to. The library parses the file, converts the parsed data
  and writes it into the area that the client provides. This option does not
  introduce a performance hit in the rendering routine, but the parsing routine
  will not provide a performance comparable to a parser specially tailored for
  this file. It also fails loading files that contain different content than the
  client expects.
\end{enumerate}

In some applications - like distributed Parallel Rendering - only a small subset
of a model is required. In the following, I will evaluate two programming
libraries that read and write PLY files in terms of their ability to load
subsets of a model. Eventually, I will propose a custom approach and demonstrate
its use.

\subsection{Motivating Example}

Say we want to parse file that in each line has three floating point
numbers specifying a position in cartesian coordinates followed by three
integers specifying a color in RGB-format. The data should be written into a
vertex type that we specify as:


Implementing a parser for this file format would be straight forward. But if we
now take a file that provides the position in double precision, or two
dimensional positions, or color with an alpha channel, or binary data, our
parser would fail to load it.




\section{Custom Approach}

The two reviewed libraries do not provide the safety and ease of use that are
desired for a parser to be used in a parallel rendering application. I propose
another approach, that is completely typesafe and requires only a minimum of
setup. It is still flexible enough to read from any kind on input like
compressed files or network streams.

My library uses the Qi parser library which allows to write grammars and format
descriptions using a format similar to Extended Backus Naur Form (EBNF) directly
in C++. The grammar to parse the PLY file header is summarized in Listing 1.

\begin{lstlisting}[float=*,frame=tb,caption=EBNF Grammar of the PLY file header]
ply      ::= "ply" EOL "format" format DOUBLE EOL element* "end_header" EOL
element  ::= "element" STRING INT EOL property*
property ::= "property" (list | scalar) STRING EOL
list     ::= "list" size scalar
format   ::= "ascii" | "binary_little_endian" | "binary_big_endian"
size     ::= "uint8" | "uint16"| "uint32" | "uint64"
scalar   ::= size | "int8" | "int16" | "int32" | "int64" | "float32" | "float64"
\end{lstlisting}

\begin{lstlisting}[float=*,frame=tb,caption=C++ Grammar of the PLY file header]
ply      %= "ply" > eol > "format" > format > double_ > eol > *element > "end_header" > eol;
element  %= "element" > *(char_ - int_) > int_ > eol > *property;
property %= "property" > (list | scalar) > *(char_ - eol) > eol;
list     %= "list" > size > scalar;
format   %= "ascii" | "binary_little_endian" | "binary_big_endian";
size     %= "uint8" | "uint16"| "uint32" | "uint64";
scalar   %= size | "int8" | "int16" | "int32" | "int64" | "float32" | "float64";
\end{lstlisting}


Building a parser from this grammar is straight forward with both Lex/YACC and
Qi. The grammar to parse the file body cannot be known at compile time and has
to be constructed after the header has been parsed. In this context Qi's
expression templates win over Lex/YACC.

Using my library, the user has to register neither types nor callback functions.
The only requirement is that the PLY elements are assigned to an object whose
type models the \emph{Sequence} concept of the Boost.Fusion library.

Any custom struct can be adapted by the macro
\texttt{BOOST\_FUSION\_ADAPT\_STRUCT} to fulfill this requirement without
modifying the struct itself . All further required information, even the
property names, are deduced by applying template metaprogramming techniques
described in \cite{Abrahams:2004:CTM:1044941} and the documentation of
Fusion\footnote{\url{http://www.boost.org/doc/libs/release/libs/fusion/}}.

\subsection{Generic Programming Techniques}

Generic programming is about writing code that is independent of the types of
objects being manipulated. The \texttt{memcpy()} function of the C standard
library is generalized at the price of type safety by the use of \texttt{void*}.
In C++, class and function templates make the generalization possible without
sacrificing type safety
\cite{Alexandrescu:2001:MCD:377789,Meyers:2005:ECS:1051335}.

The set of requirements consisting of valid expressions, associated types,
invariants and complexity guarantees is summarized as a \emph{concept}. A type
that satisfies the requirements is said to \emph{model the concept}. A concept
can extend the requirements of another concept, which is called
\emph{refinement} \cite{gregor08:devx_concepts}. The concepts used in the C++
Standard Library are documented at the SGI STL
site\footnote{\url{http://www.sgi.com/tech/stl/table_of_contents.html}}.

The function \texttt{std::copy()} copies a range of data of a generic type,
provided that the input parameters model \emph{Input Iterator} and the output
parameter models \emph{Output Iterator}. To extract a chunk, one can either
\begin{itemize}
  \item increment the input parameter to the desired begin of the chunk and then
  copy all data before the desired end or
  \item provide a custom output iterator type that skips data before the chunk.
\end{itemize}

Loading different chunks successively is a multipass algorithm. Multipass
algorithms require the iterators to model \emph{Forward Iterator} which is a
refinement of \emph{Input Iterator}.

Iterators of the C++ Standard Library that read from an input stream model the
\emph{Input Iterator} concept. The class \texttt{boost::spirit::multi\_pass} of
the Boost.Spirit library wraps any input iterator and models \emph{Forward
Iterator} by the use of reference counted buffers.

This class should be used with care. It is intended to support backtracking in
parsing grammars that contain alternatives. Improper use of this class can lead
to buffering the complete stream in the worst case.

\subsection{Reused and reusable parts}

The implementation of the library contains parts that are not specific to
parsing PLY files, but can be reused to implement parsers that load files with
similar content but different grammar:

\begin{itemize}
  \item A parser class that wraps a grammar and a range of \emph{Forward
  Iterator}s. The class provides an invocation function that parses the
  grammatical expression once.
  \item A wrapper class around such a parser that models the \emph{Input
  Iterator} concept. The wrapped parser is invocated each time the iterator is
  incremented. It allows to copy parsed data to any output while parsing on the
  fly.
  \item A model of \emph{Output Iterator} that wraps another \emph{Output
  Iterator} and two indices defining the \emph{range of interest}. Outside the
  range of interest the wrapped iterator is not incremented and assignments are
  silently consumed. This class can be used to extract chunks from a stream.
\end{itemize}

A few other reusable components that have been used in the implementation
already are available in different libraries:

\begin{itemize}
  \item Streams that decompress data on the fly are provided by Boost.Iostreams.
  \item A class that wraps an \emph{Input Iterator} range and models the
  \emph{Forward Iterator} concept is provided by Boost.Spirit.
  \item An iterator class that stores information about the current position to
  provide helpful error messages when parsing fails is provided by Boost.Spirit.
\end{itemize}

\section{Example use of the library}

Given you want to parse the vertex element of PLY files into a struct composed
of three Cartesian coordinates and three color components. To use this struct in
the proposed library, you need to adapt it as a Boost.Fusion sequence as shown
in Listing 2. Note that no change is required to the struct declaration itself.

\begin{lstlisting}[frame=tb,caption=Adapting a vertex struct]
struct vertex {
	float x, y, z;
	unsigned char red, green, blue;
};

BOOST_FUSION_ADAPT_STRUCT(vertex,
	(float, x)
	(float, y)
	(float, z)
	(unsigned char, red)
	(unsigned char, green)
	(unsigned char, blue)
)
\end{lstlisting}

The actual use of the library is shown in Listing 3. A \texttt{ply\_loader}
object is created and given the filename of the file to parse. The
\texttt{ply\_loader} can be configured by two \emph{policies}. Information about
policy base programming can be found in \cite{Alexandrescu:2001:MCD:377789}. The
\emph{error reporting policy} controls the detail level of errors to be reported
while the \emph{decompression policy} controls the stream decompression.

The two policies affect the iterator type that is used internally. The detailed
error reporting policy requires the current line of the file to be stored. When
this behavior is not desired, it can be turned off simply by using
\texttt{error\_simple} instead. The optional file decompression policy enables
bzip2, gzip and zlib decompression of streams. No decompression is performed
when this template parameter is omitted.

\begin{lstlisting}[frame=tb,caption=Example use of the library]
ply_loader<error_detailed, decompress_gzip> loader("bunny.ply.gz");
std::vector<vertex> vertices;
loader.parse_element(0, std::back_inserter(vertices));
\end{lstlisting}

The vertex definition of the Stanford Bunny's file header is shown in Listing
4. The properties do not match the declaration of the struct in Listing 2. The
library constructs a grammar at runtime that matches five consecutive floats
while the first three values are assigned to the coordinates, and the remaining
two are silently ignored. The colors will not be set. The colors can be set to a
default value, by defining a default constructor in the vertex struct of Listing
2.

\begin{lstlisting}[frame=tb,
  caption=Stanford Bunny's definition of the vertex element]
element vertex 35947
property float x
property float y
property float z
property float confidence
property float intensity
\end{lstlisting}

\section{Conclusion}

Two parsing libraries for PLY files have been analyzed. Neither of them could
provide the flexibility, ease of use and safety to make it usable as a file
loader for a distributed rendering application.

The proposed library does not sacrifice type safety while being extremely
generic. Using it does not require the registration of any callback functions
and is thus much less complex  than using libply.

The implementation itself is extremely complex. While completely abstracted from
the programmer, the library needs to handle expressions for all possible
property types. This provides very high flexibility. The impact in performance
of this complexity still has to be investigated. It is also questionable whether
this high flexibility is really required.

As stated before, implementing a loader for a given PLY file is an easy task
compared to implementing a reusable parser library. A reusable parser library
provides worse performance compared to a specifically tailored parser and
negligible benefit: It still fails to load files that contain different content
than the client expects.

An alternative approach would be to write a program that creates a parser
library for a specific file by evaluating its header. This parser library
generator then could be included in the build process of a rendering
application. It would facilitate the development of PLY parsers, while still
providing optimal performance.

%\bibliographystyle{plain}
%\bibliography{junkload}

\end{document}
